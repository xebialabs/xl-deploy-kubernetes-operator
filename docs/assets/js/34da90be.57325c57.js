"use strict";(self.webpackChunkdocumentation=self.webpackChunkdocumentation||[]).push([[693],{3905:function(e,a,t){t.d(a,{Zo:function(){return p},kt:function(){return u}});var i=t(7294);function l(e,a,t){return a in e?Object.defineProperty(e,a,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[a]=t,e}function n(e,a){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);a&&(i=i.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),t.push.apply(t,i)}return t}function r(e){for(var a=1;a<arguments.length;a++){var t=null!=arguments[a]?arguments[a]:{};a%2?n(Object(t),!0).forEach((function(a){l(e,a,t[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):n(Object(t)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(t,a))}))}return e}function o(e,a){if(null==e)return{};var t,i,l=function(e,a){if(null==e)return{};var t,i,l={},n=Object.keys(e);for(i=0;i<n.length;i++)t=n[i],a.indexOf(t)>=0||(l[t]=e[t]);return l}(e,a);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);for(i=0;i<n.length;i++)t=n[i],a.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(l[t]=e[t])}return l}var d=i.createContext({}),s=function(e){var a=i.useContext(d),t=a;return e&&(t="function"==typeof e?e(a):r(r({},a),e)),t},p=function(e){var a=s(e.components);return i.createElement(d.Provider,{value:a},e.children)},c={inlineCode:"code",wrapper:function(e){var a=e.children;return i.createElement(i.Fragment,{},a)}},m=i.forwardRef((function(e,a){var t=e.components,l=e.mdxType,n=e.originalType,d=e.parentName,p=o(e,["components","mdxType","originalType","parentName"]),m=s(t),u=l,k=m["".concat(d,".").concat(u)]||m[u]||c[u]||n;return t?i.createElement(k,r(r({ref:a},p),{},{components:t})):i.createElement(k,r({ref:a},p))}));function u(e,a){var t=arguments,l=a&&a.mdxType;if("string"==typeof e||l){var n=t.length,r=new Array(n);r[0]=m;var o={};for(var d in a)hasOwnProperty.call(a,d)&&(o[d]=a[d]);o.originalType=e,o.mdxType="string"==typeof e?e:l,r[1]=o;for(var s=2;s<n;s++)r[s]=t[s];return i.createElement.apply(null,r)}return i.createElement.apply(null,t)}m.displayName="MDXCreateElement"},7086:function(e,a,t){t.r(a),t.d(a,{frontMatter:function(){return o},contentTitle:function(){return d},metadata:function(){return s},toc:function(){return p},default:function(){return m}});var i=t(7462),l=t(3366),n=(t(7294),t(3905)),r=["components"],o={sidebar_position:15},d="Manual helm to operator upgrade of xld from version 10 to above 22.1 version.",s={unversionedId:"manual/manual-upgrade-for-xld-10",id:"manual/manual-upgrade-for-xld-10",isDocsHomePage:!1,title:"Manual helm to operator upgrade of xld from version 10 to above 22.1 version.",description:"This is internal documentation. This document can be used only if it was recommended by the Support Team.",source:"@site/docs/manual/manual-upgrade-for-xld-10.md",sourceDirName:"manual",slug:"/manual/manual-upgrade-for-xld-10",permalink:"/xl-deploy-kubernetes-operator/docs/manual/manual-upgrade-for-xld-10",tags:[],version:"current",sidebarPosition:15,frontMatter:{sidebar_position:15},sidebar:"tutorialSidebar",previous:{title:"How to change namespace of the PVC",permalink:"/xl-deploy-kubernetes-operator/docs/manual/move_pvc_to_other_namespace"}},p=[{value:"Prerequisites",id:"prerequisites",children:[],level:2},{value:"1. Backup everything",id:"1-backup-everything",children:[],level:2},{value:"2. Be sure to not delete PVs with your actions and update PV  RECLAIM POLICY To Retain",id:"2-be-sure-to-not-delete-pvs-with-your-actions-and-update-pv--reclaim-policy-to-retain",children:[],level:2},{value:"3. Creating new PVC for dai-deploy master by copying PV data.",id:"3-creating-new-pvc-for-dai-deploy-master-by-copying-pv-data",children:[{value:"i. Make the copy of the pvc-xld-production-digitalai-deploy.yaml for the later reference.",id:"i-make-the-copy-of-the-pvc-xld-production-digitalai-deployyaml-for-the-later-reference",children:[],level:3},{value:"ii. Manually create pvc data-dir-dai-xld-digitalai-deploy-master-0 as mentioned below.",id:"ii-manually-create-pvc-data-dir-dai-xld-digitalai-deploy-master-0-as-mentioned-below",children:[],level:3},{value:"iii.  Create PVC dai-xld-digitalai-deploy.",id:"iii--create-pvc-dai-xld-digitalai-deploy",children:[],level:3},{value:"iv. Verify if PV bounded",id:"iv-verify-if-pv-bounded",children:[],level:3},{value:"v. Update the Reclaim policy to Retain, for newly created pv of data-dir-dai-xld-digitalai-deploy-master-0.",id:"v-update-the-reclaim-policy-to-retain-for-newly-created-pv-of-data-dir-dai-xld-digitalai-deploy-master-0",children:[],level:3},{value:"vi. Start the following pod for accessing the newly created PVC data-dir-dai-xld-digitalai-deploy-master-0.",id:"vi-start-the-following-pod-for-accessing-the-newly-created-pvc-data-dir-dai-xld-digitalai-deploy-master-0",children:[],level:3},{value:"vii. Copy data and Give Persmission.",id:"vii-copy-data-and-give-persmission",children:[],level:3},{value:"viii.  Delete the pod.",id:"viii--delete-the-pod",children:[],level:3}],level:2},{value:"4. Creating new PVC for dai-deploy worker by copying PV data.",id:"4-creating-new-pvc-for-dai-deploy-worker-by-copying-pv-data",children:[{value:"i. Make the copy of the pvc-work-dir-xld-production-digitalai-deploy-worker-0.yaml for the later reference.",id:"i-make-the-copy-of-the-pvc-work-dir-xld-production-digitalai-deploy-worker-0yaml-for-the-later-reference",children:[],level:3},{value:"ii. Manually create pvc data-dir-dai-xld-digitalai-deploy-worker-0 as mentioned below.",id:"ii-manually-create-pvc-data-dir-dai-xld-digitalai-deploy-worker-0-as-mentioned-below",children:[],level:3},{value:"iii.  Create PVC dai-xld-digitalai-deploy.",id:"iii--create-pvc-dai-xld-digitalai-deploy-1",children:[],level:3},{value:"iv. Verify if PV bounded",id:"iv-verify-if-pv-bounded-1",children:[],level:3},{value:"v. Update the Reclaim policy to Retain, for newly created pv of data-dir-dai-xld-digitalai-deploy-worker-0.",id:"v-update-the-reclaim-policy-to-retain-for-newly-created-pv-of-data-dir-dai-xld-digitalai-deploy-worker-0",children:[],level:3},{value:"vi. Start the following pod for accessing the newly created PVC data-dir-dai-xld-digitalai-deploy-worker-0.",id:"vi-start-the-following-pod-for-accessing-the-newly-created-pvc-data-dir-dai-xld-digitalai-deploy-worker-0",children:[],level:3},{value:"vii. Copy data and Give Persmission.",id:"vii-copy-data-and-give-persmission-1",children:[],level:3},{value:"viii.  Delete the pod.",id:"viii--delete-the-pod-1",children:[],level:3}],level:2},{value:"5. Run upgrade with dry run, with custom zip options.",id:"5-run-upgrade-with-dry-run-with-custom-zip-options",children:[],level:2},{value:"5. Take backup of existing password.",id:"5-take-backup-of-existing-password",children:[],level:2},{value:"6. Do following changes in the xebialabs/dai-deploy/daideploy_cr.yaml, based on the requirement.",id:"6-do-following-changes-in-the-xebialabsdai-deploydaideploy_cryaml-based-on-the-requirement",children:[{value:"i. To update admin password",id:"i-to-update-admin-password",children:[],level:3},{value:"ii. If we are using external database.",id:"ii-if-we-are-using-external-database",children:[],level:3},{value:"iii. To setup haproxy/nginx.",id:"iii-to-setup-haproxynginx",children:[],level:3},{value:"iii. To setup oidc",id:"iii-to-setup-oidc",children:[],level:3},{value:"iv. To update the rabbitmq Persistence storageclass.",id:"iv-to-update-the-rabbitmq-persistence-storageclass",children:[],level:3},{value:"v. To reuse existing claim for postgres/rabbitmq",id:"v-to-reuse-existing-claim-for-postgresrabbitmq",children:[],level:3}],level:2},{value:"7. Bring up the xl-deploy in docker.",id:"7-bring-up-the-xl-deploy-in-docker",children:[],level:2},{value:"8. Uninstall helm.",id:"8-uninstall-helm",children:[],level:2},{value:"9. Below steps are specific to AWS EKS Cluster, with SSO access.",id:"9-below-steps-are-specific-to-aws-eks-cluster-with-sso-access",children:[{value:"1. Verify if we have clusterRole configured with assumeRole trustPolicy.",id:"1-verify-if-we-have-clusterrole-configured-with-assumerole-trustpolicy",children:[],level:3},{value:"2. Configure IAM users or roles to your Amazon EKS cluster.",id:"2-configure-iam-users-or-roles-to-your-amazon-eks-cluster",children:[],level:3}],level:2},{value:"10. Run the following command.",id:"10-run-the-following-command",children:[],level:2},{value:"11. Verify the PVC and PV.",id:"11-verify-the-pvc-and-pv",children:[],level:2}],c={toc:p};function m(e){var a=e.components,t=(0,l.Z)(e,r);return(0,n.kt)("wrapper",(0,i.Z)({},c,t,{components:a,mdxType:"MDXLayout"}),(0,n.kt)("h1",{id:"manual-helm-to-operator-upgrade-of-xld-from-version-10-to-above-221-version"},"Manual helm to operator upgrade of xld from version 10 to above 22.1 version."),(0,n.kt)("div",{className:"admonition admonition-caution alert alert--warning"},(0,n.kt)("div",{parentName:"div",className:"admonition-heading"},(0,n.kt)("h5",{parentName:"div"},(0,n.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,n.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"16",height:"16",viewBox:"0 0 16 16"},(0,n.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"}))),"caution")),(0,n.kt)("div",{parentName:"div",className:"admonition-content"},(0,n.kt)("p",{parentName:"div"},"This is internal documentation. This document can be used only if it was recommended by the Support Team."))),(0,n.kt)("div",{className:"admonition admonition-caution alert alert--warning"},(0,n.kt)("div",{parentName:"div",className:"admonition-heading"},(0,n.kt)("h5",{parentName:"div"},(0,n.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,n.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"16",height:"16",viewBox:"0 0 16 16"},(0,n.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"}))),"caution")),(0,n.kt)("div",{parentName:"div",className:"admonition-content"},(0,n.kt)("p",{parentName:"div"},"This setup is deprecated from the 22.3 version."))),(0,n.kt)("h2",{id:"prerequisites"},"Prerequisites"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"The kubectl command-line tool"),(0,n.kt)("li",{parentName:"ul"},"Access to a Kubernetes cluster with installed Deploy in the ",(0,n.kt)("inlineCode",{parentName:"li"},"default")," namespace")),(0,n.kt)("p",null,"Tested with:"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Deploy helm based installer version 10.0.0 with external database."),(0,n.kt)("li",{parentName:"ul"},"xl-deploy 10.0 upgraded to 22.1.4 with keycloak disabled."),(0,n.kt)("li",{parentName:"ul"},"xl-cli 22.1.4"),(0,n.kt)("li",{parentName:"ul"},"Aws EKS cluster.")),(0,n.kt)("h2",{id:"1-backup-everything"},"1. Backup everything"),(0,n.kt)("div",{className:"admonition admonition-caution alert alert--warning"},(0,n.kt)("div",{parentName:"div",className:"admonition-heading"},(0,n.kt)("h5",{parentName:"div"},(0,n.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,n.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"16",height:"16",viewBox:"0 0 16 16"},(0,n.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"}))),"caution")),(0,n.kt)("div",{parentName:"div",className:"admonition-content"},(0,n.kt)("p",{parentName:"div"},"Before doing any of the following steps backup everything:"),(0,n.kt)("ul",{parentName:"div"},(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("p",{parentName:"li"},(0,n.kt)("a",{parentName:"p",href:"https://docs.xebialabs.com/v.22.1/deploy/concept/the-xl-deploy-work-directory/#clean-up-the-work-directory"},"Clean up deploy Work directory"))),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("p",{parentName:"li"},"database data")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("p",{parentName:"li"},"any custom configuration that was done for the operator setup"),(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"StatefulSets"),(0,n.kt)("li",{parentName:"ul"},"Deployments"),(0,n.kt)("li",{parentName:"ul"},"ConfigMaps"),(0,n.kt)("li",{parentName:"ul"},"Secrets"),(0,n.kt)("li",{parentName:"ul"},"CustomResource"),(0,n.kt)("li",{parentName:"ul"},"anything else that was customized"))),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("p",{parentName:"li"},"any volume related to deploy master in the default namespace, for example data from the mounted volumes on deploy master pod:"),(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"/opt/xebialabs/xl-deploy-server/export"))),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("p",{parentName:"li"},"any volume related to deploy worker in the default namespace, for example data from the mounted volumes on deploy worker pod:"),(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"/opt/xebialabs/xl-deploy-server/work   ")))))),(0,n.kt)("h2",{id:"2-be-sure-to-not-delete-pvs-with-your-actions-and-update-pv--reclaim-policy-to-retain"},"2. Be sure to not delete PVs with your actions and update PV  RECLAIM POLICY To Retain"),(0,n.kt)("p",null,"Patch the all PVs to set the \u201cpersistentVolumeReclaimPolicy\u201d to \u201cRetain\u201d, for example (if cluster admin's didn't do that already):"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-shell"},'> kubectl get pv\nNAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                                                       STORAGECLASS          REASON   AGE\npvc-0e0a6bcd-77e3-4002-a355-2e5a74f637c4   10Gi       RWO            Delete           Bound    default/xld-production-digitalai-deploy                     aws-efs-provisioner            9s\npvc-5728a562-69a3-4fdd-90f8-b5316f7460a0   8Gi        RWO            Delete           Bound    default/data-xld-production-rabbitmq-0                      aws-efs-provisioner            7s\npvc-ef01748d-1d1a-46bf-8e0f-e946a8274b56   5Gi        RWO            Delete           Bound    default/work-dir-xld-production-digitalai-deploy-worker-0   aws-efs-provisioner            7s\n...\n\n> kubectl patch pv pvc-0e0a6bcd-77e3-4002-a355-2e5a74f637c4 -p \'{"spec":{"persistentVolumeReclaimPolicy":"Retain"}}\'\npersistentvolume/pvc-0e0a6bcd-77e3-4002-a355-2e5a74f637c4 patched\n')),(0,n.kt)("p",null,"Export the current PVCs objects because it will be necessary to recreate the PVCs in a later stage:"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-shell"},"> kubectl get pvc xld-production-digitalai-deploy -n default -o yaml > pvc-xld-production-digitalai-deploy.yaml\n\n> kubectl get pvc work-dir-xld-production-digitalai-deploy-worker-0 -n default -o yaml > pvc-work-dir-xld-production-digitalai-deploy-worker-0.yaml\n")),(0,n.kt)("p",null,"Iterate on all PVs that are connected to the XLD PVCs in the default namespace, list depends on the installed components.\nFor example, here is list of PVCs that are usually in the default namespace:"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"xld-production-digitalai-deploy"),(0,n.kt)("li",{parentName:"ul"},"work-dir-xld-production-digitalai-deploy-worker-0"),(0,n.kt)("li",{parentName:"ul"},"data-xld-production-postgresql-0 -- if we are using embedded database"),(0,n.kt)("li",{parentName:"ul"},"data-xld-production-rabbitmq-0")),(0,n.kt)("p",null,"On the end check if all PVs have correct Reclaim Policy:"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-shell"},"> kubectl get pv\nNAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                                                       STORAGECLASS          REASON   AGE\npvc-0e0a6bcd-77e3-4002-a355-2e5a74f637c4   10Gi       RWO            Retain           Bound    default/xld-production-digitalai-deploy                     aws-efs-provisioner            2m37s\npvc-5728a562-69a3-4fdd-90f8-b5316f7460a0   8Gi        RWO            Retain           Bound    default/data-xld-production-rabbitmq-0                      aws-efs-provisioner            2m35s\npvc-ef01748d-1d1a-46bf-8e0f-e946a8274b56   5Gi        RWO            Retain           Bound    default/work-dir-xld-production-digitalai-deploy-worker-0   aws-efs-provisioner            2m35s\n\n")),(0,n.kt)("h2",{id:"3-creating-new-pvc-for-dai-deploy-master-by-copying-pv-data"},"3. Creating new PVC for dai-deploy master by copying PV data."),(0,n.kt)("div",{className:"admonition admonition-note alert alert--secondary"},(0,n.kt)("div",{parentName:"div",className:"admonition-heading"},(0,n.kt)("h5",{parentName:"div"},(0,n.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,n.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,n.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))),"note")),(0,n.kt)("div",{parentName:"div",className:"admonition-content"},(0,n.kt)("p",{parentName:"div"},"eg: helm release name : xld-production"))),(0,n.kt)("h3",{id:"i-make-the-copy-of-the-pvc-xld-production-digitalai-deployyaml-for-the-later-reference"},"i. Make the copy of the pvc-xld-production-digitalai-deploy.yaml for the later reference."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-shell"},"> kubectl get pvc xld-production-digitalai-deploy -o yaml > pvc-xld-production-digitalai-deploy.yaml.\n")),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Copy the pvc-xld-production-digitalai-deploy.yaml file  to pvc-data-dir-dai-xld-digitalai-deploy-master-0.yaml",(0,n.kt)("pre",{parentName:"li"},(0,n.kt)("code",{parentName:"pre",className:"language-shell"}," > cp pvc-xld-production-digitalai-deploy.yaml pvc-data-dir-dai-xld-digitalai-deploy-master-0.yaml\n")))),(0,n.kt)("h3",{id:"ii-manually-create-pvc-data-dir-dai-xld-digitalai-deploy-master-0-as-mentioned-below"},"ii. Manually create pvc data-dir-dai-xld-digitalai-deploy-master-0 as mentioned below."),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Delete all the lines under sections:",(0,n.kt)("pre",{parentName:"li"},(0,n.kt)("code",{parentName:"pre",className:"language-shell"},"status\nspec.volumneMode\nspec.volumneName\nmetadata.uid\nmetadata.resourceVersion\nmetadata.namespace\nmetadata.creationTimestamp\nmetadata.finalizers\nmetadata.annotations.pv.kubernetes.io/bind-completed\nmetadata.annotations.pv.kubernetes.io/bound-by-controller\n"))),(0,n.kt)("li",{parentName:"ul"},"Update the following in yaml:",(0,n.kt)("pre",{parentName:"li"},(0,n.kt)("code",{parentName:"pre",className:"language-shell"},"metadata.name from <release-name>-digitalai-deploy to data-dir-dai-xld-digitalai-deploy-master-0\nmetadata.labels.release: dai-xld\nmetadata.annotations.meta.helm.sh/release-namespace: default\nmetadata.annotations.meta.helm.sh/release-name : dai-xld\nmetadata.annotations:helm.sh/resource-policy: keep\n")))),(0,n.kt)("h3",{id:"iii--create-pvc-dai-xld-digitalai-deploy"},"iii.  Create PVC dai-xld-digitalai-deploy."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-shell"},"> kubectl apply -f pvc-data-dir-dai-xld-digitalai-deploy-master-0.yaml\n")),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-shell"},"> kubectl apply -f pvc-data-dir-dai-xld-digitalai-deploy-master-0.yaml \npersistentvolumeclaim/data-dir-dai-xld-digitalai-deploy-master-0 created\n")),(0,n.kt)("h3",{id:"iv-verify-if-pv-bounded"},"iv. Verify if PV bounded"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-shell"},"> kubectl get pvc data-dir-dai-xld-digitalai-deploy-master-0\nNAME                                         STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS          AGE\ndata-dir-dai-xld-digitalai-deploy-master-0   Bound    pvc-64444985-8e98-406c-984f-1d12449f039f   10Gi       RWO            aws-efs-provisioner   62s\n\n\n> kubectl get pv pvc-64444985-8e98-406c-984f-1d12449f039f\nNAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                                                STORAGECLASS          REASON   AGE\npvc-64444985-8e98-406c-984f-1d12449f039f   10Gi       RWO            Delete           Bound    default/data-dir-dai-xld-digitalai-deploy-master-0   aws-efs-provisioner            2m54s\n")),(0,n.kt)("h3",{id:"v-update-the-reclaim-policy-to-retain-for-newly-created-pv-of-data-dir-dai-xld-digitalai-deploy-master-0"},"v. Update the Reclaim policy to Retain, for newly created pv of data-dir-dai-xld-digitalai-deploy-master-0."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-shell"},'> kubectl patch pv pvc-64444985-8e98-406c-984f-1d12449f039f -p \'{"spec":{"persistentVolumeReclaimPolicy":"Retain"}}\';\npersistentvolume/pvc-64444985-8e98-406c-984f-1d12449f039f patched\n \n> kubectl get pv pvc-64444985-8e98-406c-984f-1d12449f039f\nNAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                                                STORAGECLASS          REASON   AGE\npvc-64444985-8e98-406c-984f-1d12449f039f   10Gi       RWO            Retain           Bound    default/data-dir-dai-xld-digitalai-deploy-master-0   aws-efs-provisioner            3m54s\n')),(0,n.kt)("h3",{id:"vi-start-the-following-pod-for-accessing-the-newly-created-pvc-data-dir-dai-xld-digitalai-deploy-master-0"},"vi. Start the following pod for accessing the newly created PVC ","[data-dir-dai-xld-digitalai-deploy-master-0]","."),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Update the pod ","[pod-data-dir-dai-xld-digitalai-deploy-master-0.yaml]"," yaml with exact volumes which we mounted in previous installation.")),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-shell"},'apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-data-dir-dai-xld-digitalai-deploy-master-0\nspec:  \n  containers:\n    - name: sleeper      \n      command: ["sleep", "1d"]\n      image: xebialabs/tiny-tools:22.2.0\n      imagePullPolicy: Always\n      volumeMounts:\n        - mountPath: /opt/xebialabs/xl-deploy-server/export\n          name: export-dir\n          subPath: export\n  restartPolicy: Never\n  volumes:\n    - name: export-dir\n      persistentVolumeClaim:\n        claimName: data-dir-dai-xld-digitalai-deploy-master-0\n')),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Start the pod")),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-shell"},"> kubectl apply -f pod-data-dir-dai-xld-digitalai-deploy-master-0.yaml\npod/pod-data-dir-dai-xld-digitalai-deploy-master-0 created\n")),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-shell"},"> kubectl get pod/pod-data-dir-dai-xld-digitalai-deploy-master-0\nNAME                                             READY   STATUS    RESTARTS   AGE\npod-data-dir-dai-xld-digitalai-deploy-master-0   1/1     Running   0          30s\n")),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Verify the mouted path is available in newly created PV.",(0,n.kt)("pre",{parentName:"li"},(0,n.kt)("code",{parentName:"pre",className:"language-shell"},"> kubectl exec -it pod/pod-data-dir-dai-xld-digitalai-deploy-master-0 -- sh\n/ # cd /opt/xebialabs/xl-deploy-server/\n/opt/xebialabs/xl-deploy-server # ls -lrt\ntotal 4\ndrwxrws--x 2 root 40136 6144 Jun 28 09:55 export\n/opt/xebialabs/xl-deploy-server #\n")))),(0,n.kt)("h3",{id:"vii-copy-data-and-give-persmission"},"vii. Copy data and Give Persmission."),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Copy data from xld-production-digitalai-deploy-master-0 to pod-data-dir-dai-xld-digitalai-deploy-master-0")),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-shell"},"> kubectl exec -n default xld-production-digitalai-deploy-master-0 -- tar cf - /opt/xebialabs/xl-deploy-server/export | kubectl exec -n default -i pod-data-dir-dai-xld-digitalai-deploy-master-0 -- tar xvf - -C /\n")),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-shell"},'eg:\n> kubectl exec -n default xld-production-digitalai-deploy-master-0 -- tar cf - /opt/xebialabs/xl-deploy-server/export | kubectl exec -n default -i pod-data-dir-dai-xld-digitalai-deploy-master-0 -- tar xvf - -C /\nDefaulted container "digitalai-deploy" out of: digitalai-deploy, fix-the-volume-permission (init)\ntar: Removing leading `/\' from member names\nopt/xebialabs/xl-deploy-server/export/\nopt/xebialabs/xl-deploy-server/export/test.txt\nopt/xebialabs/xl-deploy-server/export/content-types/\n\n')),(0,n.kt)("div",{className:"admonition admonition-note alert alert--secondary"},(0,n.kt)("div",{parentName:"div",className:"admonition-heading"},(0,n.kt)("h5",{parentName:"div"},(0,n.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,n.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,n.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))),"note")),(0,n.kt)("div",{parentName:"div",className:"admonition-content"},(0,n.kt)("p",{parentName:"div"},"  With the Latest version 22.1.4 we are not volume mounting the /opt/xebialabs/xl-deploy-server/export folder, if required , we need to do it manually by editing the statefulset of master pod.\nand add the mount path"),(0,n.kt)("pre",{parentName:"div"},(0,n.kt)("code",{parentName:"pre",className:"language-shell"},"> kubectl edit statefulset.apps/dai-xld-digitalai-deploy-master\n")),(0,n.kt)("pre",{parentName:"div"},(0,n.kt)("code",{parentName:"pre",className:"language-yaml"},"- mountPath: /opt/xebialabs/xl-deploy-server/export\n  name: data-dir\n  subPath: export\n")))),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Give full Permission to the copied data in new PV.",(0,n.kt)("pre",{parentName:"li"},(0,n.kt)("code",{parentName:"pre",className:"language-shell"},"> kubectl exec -n default -i pod-data-dir-dai-xld-digitalai-deploy-master-0 -- chmod -R 777 /opt/xebialabs/xl-deploy-server/export\n")))),(0,n.kt)("blockquote",null,(0,n.kt)("p",{parentName:"blockquote"},"kubectl exec -it -n default pod-data-dir-dai-xld-digitalai-deploy-master-0 -- sh\n/ # cd /opt/xebialabs/xl-deploy-server/\n/opt/xebialabs/xl-deploy-server # ls -lrt\ntotal 4\ndrwxrwsrwx 3 10001 40133 6144 Jun 28 10:00 export\n/opt/xebialabs/xl-deploy-server # "),(0,n.kt)("pre",{parentName:"blockquote"},(0,n.kt)("code",{parentName:"pre"},""))),(0,n.kt)("h3",{id:"viii--delete-the-pod"},"viii.  Delete the pod."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-shell"},'> kubectl delete pod/pod-data-dir-dai-xld-digitalai-deploy-master-0\npod "pod-data-dir-dai-xld-digitalai-deploy-master-0" deleted\n')),(0,n.kt)("h2",{id:"4-creating-new-pvc-for-dai-deploy-worker-by-copying-pv-data"},"4. Creating new PVC for dai-deploy worker by copying PV data."),(0,n.kt)("div",{className:"admonition admonition-note alert alert--secondary"},(0,n.kt)("div",{parentName:"div",className:"admonition-heading"},(0,n.kt)("h5",{parentName:"div"},(0,n.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,n.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,n.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))),"note")),(0,n.kt)("div",{parentName:"div",className:"admonition-content"},(0,n.kt)("p",{parentName:"div"},"eg: helm release name : xld-production"))),(0,n.kt)("h3",{id:"i-make-the-copy-of-the-pvc-work-dir-xld-production-digitalai-deploy-worker-0yaml-for-the-later-reference"},"i. Make the copy of the pvc-work-dir-xld-production-digitalai-deploy-worker-0.yaml for the later reference."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-shell"},"> kubectl get pvc work-dir-xld-production-digitalai-deploy-worker-0 -o yaml > pvc-work-dir-xld-production-digitalai-deploy-worker-0.yaml\n")),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Copy the pvc-work-dir-xld-production-digitalai-deploy-worker-0.yaml file  to pvc-data-dir-dai-xld-digitalai-deploy-worker-0.yaml",(0,n.kt)("pre",{parentName:"li"},(0,n.kt)("code",{parentName:"pre",className:"language-shell"}," > cp pvc-work-dir-xld-production-digitalai-deploy-worker-0.yaml pvc-data-dir-dai-xld-digitalai-deploy-worker-0.yaml\n")))),(0,n.kt)("h3",{id:"ii-manually-create-pvc-data-dir-dai-xld-digitalai-deploy-worker-0-as-mentioned-below"},"ii. Manually create pvc data-dir-dai-xld-digitalai-deploy-worker-0 as mentioned below."),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Delete all the lines under sections:",(0,n.kt)("pre",{parentName:"li"},(0,n.kt)("code",{parentName:"pre",className:"language-shell"},"status\nspec.volumneMode\nspec.volumneName\nmetadata.uid\nmetadata.resourceVersion\nmetadata.namespace\nmetadata.creationTimestamp\nmetadata.finalizers\nmetadata.annotations.pv.kubernetes.io/bind-completed\nmetadata.annotations.pv.kubernetes.io/bound-by-controller\n"))),(0,n.kt)("li",{parentName:"ul"},"Update the following in yaml:",(0,n.kt)("pre",{parentName:"li"},(0,n.kt)("code",{parentName:"pre",className:"language-shell"},"metadata.name from work-dir-<release-name>-digitalai-deploy-worker-0 to data-dir-dai-xld-digitalai-deploy-worker-0\nmetadata.labels.release: dai-xld\nmetadata.annotations.meta.helm.sh/release-namespace: default\nmetadata.annotations.meta.helm.sh/release-name : dai-xld\nmetadata.annotations:helm.sh/resource-policy: keep\n")))),(0,n.kt)("h3",{id:"iii--create-pvc-dai-xld-digitalai-deploy-1"},"iii.  Create PVC dai-xld-digitalai-deploy."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-shell"},"> kubectl apply -f pvc-data-dir-dai-xld-digitalai-deploy-worker-0.yaml\n")),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-shell"},"> kubectl apply -f pvc-data-dir-dai-xld-digitalai-deploy-worker-0.yaml\npersistentvolumeclaim/data-dir-dai-xld-digitalai-deploy-worker-0 created\n")),(0,n.kt)("h3",{id:"iv-verify-if-pv-bounded-1"},"iv. Verify if PV bounded"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-shell"},"> kubectl get pvc data-dir-dai-xld-digitalai-deploy-worker-0\nNAME                                         STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS          AGE\ndata-dir-dai-xld-digitalai-deploy-worker-0   Bound    pvc-5ad67b27-027c-4636-b2b6-8a1e288c6251   5Gi        RWO            aws-efs-provisioner   27s\n\n> kubectl get pv pvc-5ad67b27-027c-4636-b2b6-8a1e288c6251\nNAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                                                STORAGECLASS          REASON   AGE\npvc-5ad67b27-027c-4636-b2b6-8a1e288c6251   5Gi        RWO            Delete           Bound    default/data-dir-dai-xld-digitalai-deploy-worker-0   aws-efs-provisioner            51s\n\n")),(0,n.kt)("h3",{id:"v-update-the-reclaim-policy-to-retain-for-newly-created-pv-of-data-dir-dai-xld-digitalai-deploy-worker-0"},"v. Update the Reclaim policy to Retain, for newly created pv of data-dir-dai-xld-digitalai-deploy-worker-0."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-shell"},'> kubectl patch pv pvc-5ad67b27-027c-4636-b2b6-8a1e288c6251 -p \'{"spec":{"persistentVolumeReclaimPolicy":"Retain"}}\';\npersistentvolume/pvc-5ad67b27-027c-4636-b2b6-8a1e288c6251 patched\n> kubectl get pv pvc-5ad67b27-027c-4636-b2b6-8a1e288c6251\n NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                               STORAGECLASS          REASON   AGE\n pvc-dad9e7c3-ae1b-4b28-b595-4f4b281a0bf2   5Gi        RWO            Retain           Bound    default/data-dir-dai-xld-digitalai-deploy-worker-0   aws-efs-provisioner            3m40s\n')),(0,n.kt)("h3",{id:"vi-start-the-following-pod-for-accessing-the-newly-created-pvc-data-dir-dai-xld-digitalai-deploy-worker-0"},"vi. Start the following pod for accessing the newly created PVC ","[data-dir-dai-xld-digitalai-deploy-worker-0]","."),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Update the pod ","[pod-data-dir-dai-xld-digitalai-deploy-worker-0.yaml]"," yaml with exact volumes which we mounted in previous installation.")),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-shell"},'apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-data-dir-dai-xld-digitalai-deploy-worker-0\nspec:  \n  containers:\n    - name: sleeper      \n      command: ["sleep", "1d"]\n      image: xebialabs/tiny-tools:22.2.0\n      imagePullPolicy: Always\n      volumeMounts:\n        - mountPath: /opt/xebialabs/xl-deploy-server/work\n          name: data-dir\n          subPath: work           \n  restartPolicy: Never\n  volumes:\n    - name: data-dir\n      persistentVolumeClaim:\n        claimName: data-dir-dai-xld-digitalai-deploy-worker-0\n')),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Start the pod")),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-shell"},"> kubectl apply -f pod-data-dir-dai-xld-digitalai-deploy-worker-0.yaml\npod/pod-data-dir-dai-xld-digitalai-deploy-worker-0 created\n")),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-shell"},"> kubectl get pod/pod-data-dir-dai-xld-digitalai-deploy-worker-0\nNAME                                             READY   STATUS    RESTARTS   AGE\npod-data-dir-dai-xld-digitalai-deploy-worker-0   1/1     Running   0          24s\n")),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Verify the mounted path is available in newly created PV.")),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-shell"},"> kubectl exec -it pod/pod-data-dir-dai-xld-digitalai-deploy-worker-0 -- sh\n/ # cd /opt/xebialabs/xl-deploy-server/\n/opt/xebialabs/xl-deploy-server # ls -lrt\ntotal 4\ndrwxrws--x 2 root 40137 6144 Jun 28 11:01 work\n/opt/xebialabs/xl-deploy-server # \n")),(0,n.kt)("h3",{id:"vii-copy-data-and-give-persmission-1"},"vii. Copy data and Give Persmission."),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Copy data from xld-production-digitalai-deploy-worker-0 to pod-data-dir-dai-xld-digitalai-deploy-worker-0")),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-shell"},"    > kubectl exec -n default xld-production-digitalai-deploy-worker-0 -- tar cf - /opt/xebialabs/xl-deploy-server/work | kubectl exec -n default -i pod-data-dir-dai-xld-digitalai-deploy-worker-0 -- tar xvf - -C /\n")),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-shell"},'eg:\n > kubectl exec -n default xld-production-digitalai-deploy-worker-0 -- tar cf - /opt/xebialabs/xl-deploy-server/work | kubectl exec -n default -i pod-data-dir-dai-xld-digitalai-deploy-worker-0 -- tar xvf - -C /\nDefaulted container "digitalai-deploy" out of: digitalai-deploy, fix-the-volume-permission (init)\ntar: Removing leading `/\' from member names\nopt/xebialabs/xl-deploy-server/work/\nopt/xebialabs/xl-deploy-server/work/task-20220628T111235796.1976/\nopt/xebialabs/xl-deploy-server/work/1d2e8d97-b433-4f92-bd56-96db8f11842c.task\n')),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Give full Permission to the copied data in new PV.")),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-shell"}," > kubectl exec -n default -i pod-data-dir-dai-xld-digitalai-deploy-worker-0 -- chmod -R 777 /opt/xebialabs/xl-deploy-server/work\n\n > kubectl exec -it -n default pod-data-dir-dai-xld-digitalai-deploy-worker-0 -- sh\n/ # cd /opt/xebialabs/xl-deploy-server/\n/opt/xebialabs/xl-deploy-server # ls -lrt\ntotal 4\ndrwxrwsrwx 3 10001 40134 6144 Jun 28 11:13 work\n/opt/xebialabs/xl-deploy-server #\n")),(0,n.kt)("h3",{id:"viii--delete-the-pod-1"},"viii.  Delete the pod."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-shell"},' > kubectl delete pod/pod-data-dir-dai-xld-digitalai-deploy-worker-0\npod "pod-data-dir-dai-xld-digitalai-deploy-worker-0" deleted\n')),(0,n.kt)("h2",{id:"5-run-upgrade-with-dry-run-with-custom-zip-options"},"5. Run upgrade with dry run, with custom zip options."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-shell"},"  > xl op --upgrade --dry-run\n")),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Sample output.")),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-shell"}," > .xl_22.1.4 op --upgrade --dry-run\n? Select the setup mode? advanced\n? Select the Kubernetes setup where the digitalai Devops Platform will be installed or uninstalled: AwsEKS [AWS EKS]\n? Do you want to use Kubernetes' current-context from ~/.kube/config? Yes\n? Do you want to use the AWS SSO credentials ? No\n? Do you want to use the AWS credentials from your ~/.aws/credentials file? Yes\n? Do you want to use an existing Kubernetes namespace? Yes\n? Enter the name of the existing Kubernetes namespace where the XebiaLabs DevOps Platform will be installed, updated or undeployed: default\nConnecting to EKS\n? Product server you want to perform upgrade for daiDeploy [Digital.ai Deploy]\n? Enter the repository name(eg: <repositoryName>/<imageName>:<tagName>) xebialabs\n? Enter the deploy server image name(eg: <repositoryName>/<imageName>:<tagName>) xl-deploy\n? Enter the image tag(eg: <repositoryName>/<imageName>:<tagName>) 22.1.4\n? Enter the deploy task engine image name for version 22 and above (eg: <repositoryName>/<imageName>:<tagName>) deploy-task-engine\n? Choose the version of the XL Deploy for Upgrader setup of operator 22.1.4\n? Use embedded keycloak? No\n? Select the type of upgrade you want. helmToOperator [Helm to Operator]\n? Operator image to use xebialabs/deploy-operator:22.1.4\n? Do you want to use custom operator zip file for Deploy? Yes\n? Deploy operator zip to use (absolute path or URL to the zip) /home/sishwarya/SprintTicket/S-84982_ns_xld_migration/helmToOperator/dryrun/deploy-operator-aws-eks-22.1.4.zip\n? Edit list of custom resource keys that will migrate to the new Deploy CR <Received>\n? Enter the helm release name. xld-production\n     -------------------------------- ----------------------------------------------------\n    | LABEL                          | VALUE                                              |\n     -------------------------------- ----------------------------------------------------\n    | AWSAccessKey                   | *****                                              |\n    | AWSAccessSecret                | *****                                              |\n    | AWSSessionToken                | *****                                              |\n    | DeployImageVersionForUpgrader  | 22.1.4                                             |\n    | EksClusterName                 | devops-operator-cluster-test-cluster               |\n    | ImageNameDeploy                | xl-deploy                                          |\n    | ImageNameDeployTaskEngine      | deploy-task-engine                                 |\n    | ImageTag                       | 22.1.4                                             |\n    | IsAwsCfgAvailable              | true                                               |\n    | K8sApiServerURL                | https://72673EC78289B3B122CAC4CA8E6473C2.gr7.us-.. |\n    | K8sSetup                       | AwsEKS                                             |\n    | Namespace                      | default                                            |\n    | OperatorImageDeployGeneric     | xebialabs/deploy-operator:22.1.4                   |\n    | OperatorZipDeploy              | /home/sishwarya/SprintTicket/S-84982_ns_xld_migr.. |\n    | OsType                         | linux                                              |\n    | PreserveCrValuesDeploy         | .spec.XldMasterCount\\n.spec.XldWorkerCount\\n.spe.. |\n    | ReleaseName                    | xld-production                                     |\n    | RepositoryName                 | xebialabs                                          |\n    | ServerType                     | daiDeploy                                          |\n    | UpgradeType                    | helmToOperator                                     |\n    | UseAWSSsoCredentials           | false                                              |\n    | UseAWSconfig                   | true                                               |\n    | UseCustomNamespace             | true                                               |\n    | UseEmbeddedKeycloak            | false                                              |\n    | UseKubeconfig                  | true                                               |\n    | UseOperatorZipDeploy           | true                                               |\n     -------------------------------- ----------------------------------------------------\n? Do you want to proceed to the deployment with these values? Yes\nGenerated files successfully! \nCreating original custom resource file...   \\ Generated files successfully helmToOperator upgrade on AwsEKS \n")),(0,n.kt)("h2",{id:"5-take-backup-of-existing-password"},"5. Take backup of existing password."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-shell"},'  ## To get the password for postgresql, run: only when we use embedded database\n   > kubectl get secret --namespace default xld-production-postgresql -o jsonpath="{.data.postgresql-password}" | base64 --decode; echo\n  \n  ## To get the admin password for xl-deploy, run:\n   >kubectl get secret --namespace default xld-production-digitalai-deploy -o jsonpath="{.data.deploy-password}" | base64 --decode; echo\n\n  ## To get the password for rabbitMQ, run:\n   > kubectl get secret xld-production-rabbitmq  -o jsonpath="{.data.rabbitmq-password}" | base64 --decode; echo\n\n')),(0,n.kt)("h2",{id:"6-do-following-changes-in-the-xebialabsdai-deploydaideploy_cryaml-based-on-the-requirement"},"6. Do following changes in the xebialabs/dai-deploy/daideploy_cr.yaml, based on the requirement."),(0,n.kt)("h3",{id:"i-to-update-admin-password"},"i. To update admin password"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},'Default  deploy admin password is "admin", if we need to update below fields.')),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-shell"},"    .spec.AdminPassword: <password from previous installation>\n")),(0,n.kt)("h3",{id:"ii-if-we-are-using-external-database"},"ii. If we are using external database."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-shell"},"    .spec.UseExistingDB.enabled: true\n    .spec.UseExistingDB.XL_DB_PASSWORD: <db password from previous installation>\n    .spec.UseExistingDB.XL_DB_URL: <db url from previous installation>\n    .spec.UseExistingDB.XL_DB_USERNAME: <db username from previous installation>\n")),(0,n.kt)("h3",{id:"iii-to-setup-haproxynginx"},"iii. To setup haproxy/nginx."),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("p",{parentName:"li"},"To enable haproxy setup   "),(0,n.kt)("pre",{parentName:"li"},(0,n.kt)("code",{parentName:"pre",className:"language-shell"},'   .spec.haproxy-ingress.install = true\n   .spec.nginx-ingress-controller.install = false\n   .spec.ingress.path = "/"\n           \n   ## in the spec.ingress.annotations replace all nginx. settings and put:\n   kubernetes.io/ingress.class: "haproxy"\n   ingress.kubernetes.io/ssl-redirect: "false"\n   ingress.kubernetes.io/rewrite-target: /\n   ingress.kubernetes.io/affinity: cookie\n   ingress.kubernetes.io/session-cookie-name: JSESSIONID\n   ingress.kubernetes.io/session-cookie-strategy: prefix\n   ingress.kubernetes.io/config-backend: |\n   option httpchk GET /ha/health HTTP/1.0\n'))),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("p",{parentName:"li"},"To enable nginx controller"),(0,n.kt)("pre",{parentName:"li"},(0,n.kt)("code",{parentName:"pre",className:"language-shell"},"   .spec.haproxy-ingress.install = false\n   .spec.nginx-ingress-controller.install = true\n"))),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("p",{parentName:"li"},"To enable external nginx or haproxy"),(0,n.kt)("pre",{parentName:"li"},(0,n.kt)("code",{parentName:"pre",className:"language-shell"},' * 1.\n   .spec.haproxy-ingress.install = false\n   .spec.nginx-ingress-controller.install = false\n\n * 2.  in .spec.ingress.annotations.\n   kubernetes.io/ingress.class: "<external ingress class>"\n\n * 3.   \n   .spec.ingress.hosts = <external nginxor haproxy hosts>\n')))),(0,n.kt)("h3",{id:"iii-to-setup-oidc"},"iii. To setup oidc"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"By default keycloak will be enabled as default oidc provider.",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("p",{parentName:"li"},"To disable oidc and keycloak."),(0,n.kt)("pre",{parentName:"li"},(0,n.kt)("code",{parentName:"pre",className:"language-shell"},"   .spec.keycloak.install = false\n   .spec.oidc.enabled =  false\n"))),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("p",{parentName:"li"},"To disable keycloak and enable external oidc."),(0,n.kt)("pre",{parentName:"li"},(0,n.kt)("code",{parentName:"pre",className:"language-shell"},"   .spec.keycloak.install = false\n   .spec.oidc.enabled =  true\n   .spec.oidc.external = true\n\n   ##  update the below fields with external oidc configuration\n   .spec.oidc.accessTokenUri:\n   .spec.oidc.clientId:\n   .spec.oidc.clientSecret:\n   .spec.oidc.emailClaim:\n   .spec.oidc.external:\n   .spec.oidc.fullNameClaim:\n   .spec.oidc.issuer:\n   .spec.oidc.keyRetrievalUri:\n   .spec.oidc.logoutUri:\n   .spec.oidc.postLogoutRedirectUri:\n   .spec.oidc.redirectUri:\n   .spec.oidc.rolesClaim:\n   .spec.oidc.userAuthorizationUri:\n   .spec.oidc..userNameClaim:\n"))),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("p",{parentName:"li"},"To enable keycloak, with default embedded database."),(0,n.kt)("div",{parentName:"li",className:"admonition admonition-caution alert alert--warning"},(0,n.kt)("div",{parentName:"div",className:"admonition-heading"},(0,n.kt)("h5",{parentName:"div"},(0,n.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,n.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"16",height:"16",viewBox:"0 0 16 16"},(0,n.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"}))),"caution")),(0,n.kt)("div",{parentName:"div",className:"admonition-content"},(0,n.kt)("p",{parentName:"div"},"  Known issue: in 22.1.4"),(0,n.kt)("pre",{parentName:"div"},(0,n.kt)("code",{parentName:"pre"},'* Issue 1:\n    * If the Previous installation using external database for xl-deploy.\n    * When we try to upgrade to latest 22.1.4 with keycloak enabled with embedded database, then we will be end using embedded database for both deploy and keycloak.\n       ".spec.UseExistingDB.enabled", has no effects.\n* Issue 2:\n    * If the Previous installation using embedded database for xl-deploy.\n    * When we try to upgrade to latest 22.1.4 with keycloak enabled with embedded database.\n        * Post upgrade keycloak pod failed to start with below error.\n        Caused by: org.postgresql.util.PSQLException: FATAL: password authentication failed for user "keycloak"\n          * We need to Connect to the pod/dai-xld-postgresql-0 pod and create the keycloak database.\n          * kuebctl exec -it pod/dai-xld-postgresql-0 -- bash\n          * psql -U postgres  \n              note : Postgres password from previous installation\n          * create database keycloak;\n          * create user keycloak with encrypted password \'keycloak\';\n          * grant all privileges on database keycloak to keycloak;            \n')))),(0,n.kt)("pre",{parentName:"li"},(0,n.kt)("code",{parentName:"pre",className:"language-shell"},"    .spec.keycloak.install = true\n    .spec.oidc.enabled =  true\n    .spec.oidc.external = false\n    .spec.postgresql.install = true\n    .spec.postgresql.persistence.storageClass = <storageClass specific to provider>\n    .spec.keycloak.ingress.console.rules[0].host = <hosts for keycloak>\n    .spec.keycloak.ingress.rules[0].host = <hosts for keycloak>\n"))),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("p",{parentName:"li"},"To enable keycloak, with external database."),(0,n.kt)("pre",{parentName:"li"},(0,n.kt)("code",{parentName:"pre"},"```shell\n            .spec.keycloak.install = true\n            .spec.oidc.enabled =  true\n            .spec.oidc.external = false\n            .spec.postgresql.install = true\n            .spec.postgresql.persistence.storageClass = <storageClass specific to provider>\n            .spec.keycloak.ingress.console.rules[0].host = <hosts for keycloak>\n            .spec.keycloak.ingress.rules[0].host = <hosts for keycloak>            \n```\n")),(0,n.kt)("p",{parentName:"li"}," We need to set the External DB in .spec.keycloak.extraEnv.\nRefer docs for more details : ",(0,n.kt)("a",{parentName:"p",href:"https://docs.digital.ai/bundle/devops-release-version-v.22.1/page/release/how-to/k8s-operator/keycloak-configuration_for_k8s_operator.html"},"keycloak-configuration_for_k8s_operator")))))),(0,n.kt)("h3",{id:"iv-to-update-the-rabbitmq-persistence-storageclass"},"iv. To update the rabbitmq Persistence storageclass."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-shell"},"   .spec.rabbitmq.persistence.storageClass: <Storage Class to be defined for RabbitMQ>\n")),(0,n.kt)("h3",{id:"v-to-reuse-existing-claim-for-postgresrabbitmq"},"v. To reuse existing claim for postgres/rabbitmq"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("p",{parentName:"li"},'If the release name is different from "dai-xld" and if we are using embedded database, we need to reuse the existing Claim, for data persistence.'),(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Update the following field with existing claim.")),(0,n.kt)("pre",{parentName:"li"},(0,n.kt)("code",{parentName:"pre",className:"language-shell"},"          .spec.postgresql.persistence.existingClaim\n          .spec.rabbitmq.persistence.existingClaim --\x3e not required, as we dont save any data.\n")),(0,n.kt)("pre",{parentName:"li"},(0,n.kt)("code",{parentName:"pre",className:"language-shell"},"eg:\n    .spec.postgresql.persistence.existingClaim: data-xld-production-postgresql-0\n")),(0,n.kt)("div",{parentName:"li",className:"admonition admonition-note alert alert--secondary"},(0,n.kt)("div",{parentName:"div",className:"admonition-heading"},(0,n.kt)("h5",{parentName:"div"},(0,n.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,n.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,n.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))),"note")),(0,n.kt)("div",{parentName:"div",className:"admonition-content"},(0,n.kt)("p",{parentName:"div"}," If we are having more than one existing PVC for rabbitmq, we don't use existingClaim for rabbitmq configuration, instead we can follow the other approach mentioned below for PV reuse."))),(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Post helm uninstall, we can also edit postgres/rabbitmq PV as follows, to create the new PVC with existing PV.",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Update the postgres pv with following details.      ",(0,n.kt)("pre",{parentName:"li"},(0,n.kt)("code",{parentName:"pre",className:"language-shell"},"            claimRef:\n              apiVersion: v1\n              kind: PersistentVolumeClaim\n              name: data-dai-xld-postgresql-0\n              namespace: default   \n"))),(0,n.kt)("li",{parentName:"ul"},"Update the rabbitmq pv with following details if we need to reuse the PV of rabbitmq.",(0,n.kt)("pre",{parentName:"li"},(0,n.kt)("code",{parentName:"pre",className:"language-shell"},"            claimRef:\n              apiVersion: v1\n              kind: PersistentVolumeClaim\n              name: data-dai-xld-rabbitmq-0\n              namespace: default   \n"))),(0,n.kt)("li",{parentName:"ul"},"Remove the following from PV ","[postgres/rabbitmq]"," while editing.",(0,n.kt)("pre",{parentName:"li"},(0,n.kt)("code",{parentName:"pre",className:"language-shell"},"           claimRef:\n            uid:\n            resourceVersion:\n")))))))),(0,n.kt)("h2",{id:"7-bring-up-the-xl-deploy-in-docker"},"7. Bring up the xl-deploy in docker."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-shell"},'  > docker run -d -e "ADMIN_PASSWORD=admin" -e "ACCEPT_EULA=Y" -p 4516:4516 --name xld xebialabs/xl-deploy:22.1.4\n')),(0,n.kt)("h2",{id:"8-uninstall-helm"},"8. Uninstall helm."),(0,n.kt)("div",{className:"admonition admonition-caution alert alert--warning"},(0,n.kt)("div",{parentName:"div",className:"admonition-heading"},(0,n.kt)("h5",{parentName:"div"},(0,n.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,n.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"16",height:"16",viewBox:"0 0 16 16"},(0,n.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"}))),"caution")),(0,n.kt)("div",{parentName:"div",className:"admonition-content"},(0,n.kt)("p",{parentName:"div"},"Before doing helm uninstall, Clean up deploy work directory:"),(0,n.kt)("ul",{parentName:"div"},(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://docs.xebialabs.com/v.22.1/deploy/concept/the-xl-deploy-work-directory/#clean-up-the-work-directory"},"Clean up deploy Work directory"))))),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-shell"}," > helm uninstall <release name>\n")),(0,n.kt)("h2",{id:"9-below-steps-are-specific-to-aws-eks-cluster-with-sso-access"},"9. Below steps are specific to AWS EKS Cluster, with SSO access."),(0,n.kt)("h3",{id:"1-verify-if-we-have-clusterrole-configured-with-assumerole-trustpolicy"},"1. Verify if we have clusterRole configured with assumeRole trustPolicy."),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Update the ClusterRole TrustPolicy with AssumeRole.",(0,n.kt)("div",{parentName:"li",className:"admonition admonition-note alert alert--secondary"},(0,n.kt)("div",{parentName:"div",className:"admonition-heading"},(0,n.kt)("h5",{parentName:"div"},(0,n.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,n.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,n.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))),"note")),(0,n.kt)("div",{parentName:"div",className:"admonition-content"},(0,n.kt)("p",{parentName:"div"}," ",(0,n.kt)("a",{parentName:"p",href:"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_create_for-user.html"},"Refer AWS Documentation - Creating a role to delegate permissions to an IAM user"),"  "))))),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-json"},' {\n            "Effect": "Allow",\n            "Principal": {\n              "AWS": "<add role or user arn here>"\n            },\n            "Action": "sts:AssumeRole"\n        }\n       \n')),(0,n.kt)("h3",{id:"2-configure-iam-users-or-roles-to-your-amazon-eks-cluster"},"2. Configure IAM users or roles to your Amazon EKS cluster."),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Update aws-auth configmap of Cluster with the RoleArn and RoleName.",(0,n.kt)("div",{parentName:"li",className:"admonition admonition-note alert alert--secondary"},(0,n.kt)("div",{parentName:"div",className:"admonition-heading"},(0,n.kt)("h5",{parentName:"div"},(0,n.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,n.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,n.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))),"note")),(0,n.kt)("div",{parentName:"div",className:"admonition-content"},(0,n.kt)("p",{parentName:"div"},(0,n.kt)("a",{parentName:"p",href:"https://docs.aws.amazon.com/eks/latest/userguide/add-user-role.html"},"Refer AWS Documentation - Add IAM users or roles to your Amazon EKS cluster")))))),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-yaml"},"mapRoles:\n----\n- groups:\n  - system:bootstrappers\n  - system:nodes\n  rolearn: <add role arn>\n  username: <add role name>\n\n")),(0,n.kt)("h2",{id:"10-run-the-following-command"},"10. Run the following command."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-shell"}," > xl apply -f xebialabs.yaml\n")),(0,n.kt)("h2",{id:"11-verify-the-pvc-and-pv"},"11. Verify the PVC and PV."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-shell"}," > kubectl get pvc\nNAME                                                STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS          AGE\ndata-dai-xld-rabbitmq-0                             Bound    pvc-eabd8a4b-8f40-4419-aed6-00f5dfe71982   8Gi        RWO            aws-efs-provisioner   2m11s\ndata-dir-dai-xld-digitalai-deploy-cc-server-0       Bound    pvc-59ef0be3-37ea-4f89-b7a5-693c2cb9ede2   500M       RWO            aws-efs-provisioner   2m11s\ndata-dir-dai-xld-digitalai-deploy-master-0          Bound    pvc-64444985-8e98-406c-984f-1d12449f039f   10Gi       RWO            aws-efs-provisioner   127m\ndata-dir-dai-xld-digitalai-deploy-worker-0          Bound    pvc-5ad67b27-027c-4636-b2b6-8a1e288c6251   5Gi        RWO            aws-efs-provisioner   65m\ndata-xld-production-rabbitmq-0                      Bound    pvc-8c0a04a8-7657-4239-9faa-dafac99f4d6e   8Gi        RWO            aws-efs-provisioner   136m\nwork-dir-xld-production-digitalai-deploy-worker-0   Bound    pvc-12c4da86-3b09-4f87-a842-c1b65cae0136   5Gi        RWO            aws-efs-provisioner   136m\n\n\n > kubectl get pv\nNAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS     CLAIM                                                       STORAGECLASS          REASON   AGE\npvc-12c4da86-3b09-4f87-a842-c1b65cae0136   5Gi        RWO            Retain           Bound      default/work-dir-xld-production-digitalai-deploy-worker-0   aws-efs-provisioner            136m\npvc-59ef0be3-37ea-4f89-b7a5-693c2cb9ede2   500M       RWO            Delete           Bound      default/data-dir-dai-xld-digitalai-deploy-cc-server-0       aws-efs-provisioner            2m48s\npvc-5ad67b27-027c-4636-b2b6-8a1e288c6251   5Gi        RWO            Retain           Bound      default/data-dir-dai-xld-digitalai-deploy-worker-0          aws-efs-provisioner            65m\npvc-64444985-8e98-406c-984f-1d12449f039f   10Gi       RWO            Retain           Bound      default/data-dir-dai-xld-digitalai-deploy-master-0          aws-efs-provisioner            128m\npvc-8c0a04a8-7657-4239-9faa-dafac99f4d6e   8Gi        RWO            Retain           Bound      default/data-xld-production-rabbitmq-0                      aws-efs-provisioner            136m\npvc-c7aa5a61-594d-4537-ad2e-dbc18d441b91   10Gi       RWO            Retain           Released   default/xld-production-digitalai-deploy                     aws-efs-provisioner            136m\npvc-eabd8a4b-8f40-4419-aed6-00f5dfe71982   8Gi        RWO            Delete           Bound      default/data-dai-xld-rabbitmq-0                             aws-efs-provisioner            2m48s\n\n")),(0,n.kt)("div",{className:"admonition admonition-note alert alert--secondary"},(0,n.kt)("div",{parentName:"div",className:"admonition-heading"},(0,n.kt)("h5",{parentName:"div"},(0,n.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,n.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,n.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))),"note")),(0,n.kt)("div",{parentName:"div",className:"admonition-content"},(0,n.kt)("p",{parentName:"div"},"Note:"),(0,n.kt)("ul",{parentName:"div"},(0,n.kt)("li",{parentName:"ul"},"We will see new PVC and PV created for rabbitmq, we can delete the old PVC and PV.",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"kubectl delete pvc data-xld-production-rabbitmq-0"),(0,n.kt)("li",{parentName:"ul"},"kubectl delete pv pvc-8c0a04a8-7657-4239-9faa-dafac99f4d6e"))),(0,n.kt)("li",{parentName:"ul"},"We can reuse the existing claim for postgres."),(0,n.kt)("li",{parentName:"ul"},"We are using newly created PVC data-dir-dai-xld-digitalai-deploy-master-0 for xl-deploy master pod."),(0,n.kt)("li",{parentName:"ul"},"We are using newly created PVC data-dir-dai-xld-digitalai-deploy-worker-0 for xl-deploy worker pod.")))))}m.isMDXComponent=!0}}]);